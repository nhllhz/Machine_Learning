{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Tabla de Contenidos<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Normalización-y-Estandarización\" data-toc-modified-id=\"Normalización-y-Estandarización-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Normalización y Estandarización</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalización-con-Scikit---Learn\" data-toc-modified-id=\"Normalización-con-Scikit---Learn-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Normalización con Scikit - Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fundamentos\" data-toc-modified-id=\"Fundamentos-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Fundamentos</a></span></li><li><span><a href=\"#Normalización-con-Scikit---learn\" data-toc-modified-id=\"Normalización-con-Scikit---learn-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Normalización con Scikit - learn</a></span></li></ul></li><li><span><a href=\"#Inclusión-de-la-Normalización-en-el-flujo-de-Machine-Learning\" data-toc-modified-id=\"Inclusión-de-la-Normalización-en-el-flujo-de-Machine-Learning-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Inclusión de la Normalización en el flujo de Machine Learning</a></span></li><li><span><a href=\"#Estandarización\" data-toc-modified-id=\"Estandarización-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Estandarización</a></span></li><li><span><a href=\"#Estandarización-con-Scikit---Learn-con-StandardScaler\" data-toc-modified-id=\"Estandarización-con-Scikit---Learn-con-StandardScaler-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Estandarización con Scikit - Learn con StandardScaler</a></span></li><li><span><a href=\"#Inclusión-de-la-&quot;estandarización&quot;en-el-flujo-de-Machine-Learning\" data-toc-modified-id=\"Inclusión-de-la-&quot;estandarización&quot;en-el-flujo-de-Machine-Learning-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Inclusión de la \"estandarización\"en el flujo de Machine Learning</a></span></li></ul></li><li><span><a href=\"#La-pregunta-del-millón:-normalizar-o-estandarizar?\" data-toc-modified-id=\"La-pregunta-del-millón:-normalizar-o-estandarizar?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>La pregunta del millón: normalizar o estandarizar?</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T23:37:09.996763Z",
     "start_time": "2020-06-22T23:37:09.938917Z"
    }
   },
   "source": [
    "![IES21](img/logo_ies.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SP04 Preprocesamiento - Cambio de Escala de las variables numéricas: Normalización y Estandarización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.267705Z",
     "start_time": "2020-06-22T13:32:13.263692Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización y Estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos modelos de Machine Learning, principalmente los que se basan en el cálculo de alguna distancia como kNN y sus derivados, y otros que calculan parámetros asociados a las variables, como por ejemplo los de Regresión Lineal y Logística o SVM, son muy sensibles a las escalas en que se han medido las variables.  Otros, como los de Árbol y sus derivados, no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprender este efecto, supongamos que en un dataset tenemos una variable, x1, que toma sus valores entre 0 y 1 y otra, x2, que toma sus valores entre 0 y 1000.   \n",
    "\n",
    "\n",
    "Supongamos que estamos usando kNN y queremos determinar qué vectores están más cerca de una nueva observación $ o=[0.1,900] $ , \n",
    "\n",
    "y queremos calcular la distancia con respecto a estas dos observaciones:\n",
    "\n",
    "$ o1=[0.9, 900] ]$  y $ o2=[0.1, 905] $ \n",
    "\n",
    "Fíjese que entre o y o1 \"están muy lejanas\" en lo que respecta a la  primer variable (porque tomaba valores entre 0 y 1), pero sin embargo o se encontrará más lejos de o2 que tiene el mismo valor en la primer variable y una diferencia 'pequeña' (5 entre 1000) en la segunda: pequeñas variaciones en la variable que asume valores más grandes \"enmascaran\" grandes diferencias en las más pequeñas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos problemas pueden solucionarse de dos maneras: **normalización** o **estandarización** de las variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes que nada: \n",
    "    \n",
    "> Tanto la normalización como la estandarización deben hacer basándose **sólo en los valores del X_train** y luego aplicarse al X_train y X_test y a las nuevas observaciones.   \n",
    "\n",
    "> Sólo se aplican a las variables numéricas, no a las categóricas. \n",
    "\n",
    "> Por otro lado la variable target (generalmente) no se normaliza ni estandariza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:29:31.009631Z",
     "start_time": "2020-06-16T15:29:31.005660Z"
    }
   },
   "source": [
    "### Normalización con Scikit - Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tenemos una variable que toma sus valores en un rango entre [xmin, xmax], normalizarla significa transformarla para que sus valores estén en otro rango, generalmente [0,1].  \n",
    "\n",
    "Por ejemplo, supongamos que tenemos una variable como x1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.387374Z",
     "start_time": "2020-06-22T13:32:13.377385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1\n",
       "0   10\n",
       "1   90\n",
       "2   80\n",
       "3   70\n",
       "4  100\n",
       "5   10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame([[10],[90],[80],[70],[100],[10]], columns=['x1'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:24:21.735023Z",
     "start_time": "2020-06-16T16:24:21.729040Z"
    }
   },
   "source": [
    "El valor más pequeño es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.410323Z",
     "start_time": "2020-06-22T13:32:13.403316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x1.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y el valor más grande es:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.434247Z",
     "start_time": "2020-06-22T13:32:13.428249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.x1.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir que sus valores se encuentran entre xmin=10 y xmax=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si quisiéramos que sus valores se encontraran entre 0 y 1, deberíamos hacer dos cosas:  \n",
    "\n",
    "1- Restar xmin a todos sus valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.463165Z",
     "start_time": "2020-06-22T13:32:13.458170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1    80\n",
       "2    70\n",
       "3    60\n",
       "4    90\n",
       "5     0\n",
       "Name: x1, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_1=df.x1-df.x1.min()\n",
    "x1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma ahora el valor más pequeño es 0, pero el más grande todavía no es 1 (en nuestro caso es 90). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Dividir sus valores por el \"rango\" (xmax - xmin).  \n",
    "\n",
    "En nuestro caso el rango es (100-10) = 90, entonces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.510030Z",
     "start_time": "2020-06-22T13:32:13.503049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000000\n",
       "1    0.888889\n",
       "2    0.777778\n",
       "3    0.666667\n",
       "4    1.000000\n",
       "5    0.000000\n",
       "Name: x1, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_2=x1_1/90\n",
    "x1_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí, sus valores están entre 0 y 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En definitiva la fórmula para normalizar al intervalo [0,1] sería:  \n",
    "\n",
    "$$ x_{inormalizado}=\\frac{x_i -x_{min}} {x_{max} - x_{min}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T14:23:37.436703Z",
     "start_time": "2020-06-22T14:23:37.431710Z"
    }
   },
   "source": [
    "- No nos olvidemos que si nuestro objetivo es pronosticar **los valores de x<sub>min</sub> y  x<sub>max</sub> son los del X_train** ya que no podemos aprender del X_test.  \n",
    "\n",
    "Esto implica que cuando apliquemos la normalización al X_train, todos los valores obtenidos estárán entre 0 y 1, pero **cuando apliquemos la normalización al X_test podríamos obtener valores por fuera de este intervalo**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:35:06.684555Z",
     "start_time": "2020-06-16T16:35:06.678573Z"
    }
   },
   "source": [
    "#### Normalización con Scikit - learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afortunadamente no tendremos que realizar estas cuentas, ya que sklearn nos provee de una opción para Normalizar: **MinMaxScaler**.   \n",
    "\n",
    "La documentación oficial se encuentra en https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "\n",
    "Su sintaxis es la siguiente:  \n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True)\n",
    "~~~  \n",
    "\n",
    "Sus opciones son:  \n",
    "\n",
    "- **feature_range**=(0, 1). El rango de valores al que queremos normalizar, generalmente será el valor por defecto que es el intervalo [0,1]  \n",
    "\n",
    "- **copy**=True Para indicar si queremos que el reemplazo se efectúe sobre el mismo array que al que transformamos o sobre una copia. Por defecto crea una copia, si queremos que cambie los valores sobre el array original debemos pasar el valor False.   \n",
    "\n",
    "\n",
    "Para que funcione, se debe aplicar sobre un array bidimensional y el resultado será un array de numpy. \n",
    "\n",
    "\n",
    "Apliquémoslo al caso anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.568873Z",
     "start_time": "2020-06-22T13:32:13.564884Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.579844Z",
     "start_time": "2020-06-22T13:32:13.570868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.88888889],\n",
       "       [0.77777778],\n",
       "       [0.66666667],\n",
       "       [1.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1), copy=False)\n",
    "scaler.fit(df)\n",
    "scaler.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusión de la Normalización en el flujo de Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Asumamos que ya hemos imputado los _missing values_.\n",
    "- Supongamos que tenemos el siguiente Dataset, ya dividido en X_train y X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.639683Z",
     "start_time": "2020-06-22T13:32:13.621732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peso</th>\n",
       "      <th>envergadura</th>\n",
       "      <th>globulos_blancos</th>\n",
       "      <th>altura</th>\n",
       "      <th>otras_enfermedades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>pequeño</td>\n",
       "      <td>normal</td>\n",
       "      <td>alto</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>medio</td>\n",
       "      <td>alto</td>\n",
       "      <td>medio</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>medio</td>\n",
       "      <td>normal</td>\n",
       "      <td>bajo</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>pequeño</td>\n",
       "      <td>alto</td>\n",
       "      <td>alto</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "      <td>medio</td>\n",
       "      <td>normal</td>\n",
       "      <td>medio</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>77</td>\n",
       "      <td>medio</td>\n",
       "      <td>normal</td>\n",
       "      <td>medio</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52</td>\n",
       "      <td>pequeño</td>\n",
       "      <td>alto</td>\n",
       "      <td>bajo</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   peso envergadura globulos_blancos altura otras_enfermedades\n",
       "0    50     pequeño           normal   alto                 Si\n",
       "1    73       medio             alto  medio                 No\n",
       "2    68       medio           normal   bajo                 No\n",
       "3    65     pequeño             alto   alto                 Si\n",
       "4    76       medio           normal  medio                 Si\n",
       "5    77       medio           normal  medio                 No\n",
       "6    52     pequeño             alto   bajo                 Si"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=pd.DataFrame([[50,'pequeño','normal','alto','Si'],[73,'medio','alto','medio','No'],[68,'medio','normal','bajo','No'],\n",
    "                 [65,'pequeño','alto','alto', 'Si'],[76,'medio','normal','medio', 'Si'],[77,'medio','normal','medio', 'No'],\n",
    "                 [52, 'pequeño','alto','bajo','Si']], \n",
    "                columns=[ 'peso','envergadura', 'globulos_blancos','altura', 'otras_enfermedades'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.657649Z",
     "start_time": "2020-06-22T13:32:13.642675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peso</th>\n",
       "      <th>envergadura</th>\n",
       "      <th>globulos_blancos</th>\n",
       "      <th>altura</th>\n",
       "      <th>otras_enfermedades</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93</td>\n",
       "      <td>grande</td>\n",
       "      <td>alto</td>\n",
       "      <td>medio</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>medio</td>\n",
       "      <td>normal</td>\n",
       "      <td>bajo</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43</td>\n",
       "      <td>pequeño</td>\n",
       "      <td>alto</td>\n",
       "      <td>alto</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86</td>\n",
       "      <td>grande</td>\n",
       "      <td>bajo</td>\n",
       "      <td>medio</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   peso envergadura globulos_blancos altura otras_enfermedades\n",
       "0    93      grande             alto  medio                 No\n",
       "1    65       medio           normal   bajo                 No\n",
       "2    43     pequeño             alto   alto                 Si\n",
       "3    86      grande             bajo  medio                 Si"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=pd.DataFrame([[93,'grande','alto','medio','No'],[65,'medio','normal','bajo','No'],\n",
    "                 [43,'pequeño','alto','alto', 'Si'],[86,'grande','bajo','medio', 'Si']], \n",
    "                columns=[ 'peso','envergadura', 'globulos_blancos','altura', 'otras_enfermedades'])\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sus variables son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T23:28:56.994592Z",
     "start_time": "2020-06-22T23:28:56.988612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['peso', 'envergadura', 'globulos_blancos', 'altura',\n",
       "       'otras_enfermedades'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- peso: numérica. La vamos a **normalizar**  \n",
    "\n",
    "- envergadura: ordinal. Vamos a **asignar su orden manualmente**.  \n",
    "- globulos_blancos: ordinal. Vamos a **asignar su orden manualmente**.  \n",
    "- altura: ordinal. Vamos a **asignar su orden manualmente**.  \n",
    "\n",
    "- otras_enfermedades: nominal. La transformaremos en **dummy variables** con OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.664618Z",
     "start_time": "2020-06-22T13:32:13.660645Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.671606Z",
     "start_time": "2020-06-22T13:32:13.667610Z"
    }
   },
   "outputs": [],
   "source": [
    "t_norm=(\"normalizador\",MinMaxScaler(feature_range=(0, 1)),['peso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.678579Z",
     "start_time": "2020-06-22T13:32:13.673593Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy para otras_enfermedades:\n",
    "t_nominal=(\"onehot\",OneHotEncoder(sparse=False),['otras_enfermedades'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T13:32:13.691545Z",
     "start_time": "2020-06-22T13:32:13.686559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ordinal con asignación de orden para envergadura,altura y globulos_blancos\n",
    "t_ordinal_enc=(\"ordinal_enc\", OrdinalEncoder(categories=[['pequeño','medio','grande'],\n",
    "                                                         ['bajo','medio','alto'],['bajo','normal','alto']]), \n",
    "               ['envergadura','altura','globulos_blancos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:41.106020Z",
     "start_time": "2020-06-22T15:29:41.101026Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "transformador_columnas= ColumnTransformer(transformers=[t_norm,t_nominal,t_ordinal_enc],\n",
    "                                                       remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:47.027943Z",
     "start_time": "2020-06-22T15:29:47.014976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformador_columnas.fit(X_train)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:50.736474Z",
     "start_time": "2020-06-22T15:29:50.726499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.        , 0.        , 2.        ,\n",
       "        1.        ],\n",
       "       [0.85185185, 1.        , 0.        , 1.        , 1.        ,\n",
       "        2.        ],\n",
       "       [0.66666667, 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        ],\n",
       "       [0.55555556, 0.        , 1.        , 0.        , 2.        ,\n",
       "        2.        ],\n",
       "       [0.96296296, 0.        , 1.        , 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [1.        , 1.        , 0.        , 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.07407407, 0.        , 1.        , 0.        , 0.        ,\n",
       "        2.        ]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformado=transformador_columnas.transform(X_train)\n",
    "X_train_transformado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para transformar el Test Set debemos utilizar el transformador \"entrenado\" en el Train Set (no nos olvidemos que no debemos aprender del Test Set antes de evaluar el modelo!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:56.706372Z",
     "start_time": "2020-06-22T15:29:56.695402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.59259259,  1.        ,  0.        ,  2.        ,  1.        ,\n",
       "         2.        ],\n",
       "       [ 0.55555556,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-0.25925926,  0.        ,  1.        ,  0.        ,  2.        ,\n",
       "         2.        ],\n",
       "       [ 1.33333333,  0.        ,  1.        ,  2.        ,  1.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformado=transformador_columnas.transform(X_test)\n",
    "X_test_transformado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos listos nuestros datos para pasar a cualquier modelo de Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se hace análisis estadístico, algunos algoritmos necesitan ciertas condiciones para funcionar correctamente, típicamente dos:\n",
    "\n",
    "- Que la media sea 0  \n",
    "- Que el  desvío standard sea 1,  $\\sigma = 1$.  \n",
    "- En algunos casos también es necesario que cada una de las variables X tenga una distribución normal o gaussiana.  \n",
    "\n",
    "Para pronosticar, no suele ser necesario ser tan estricto y generalmente conseguir que nuestras variables numéricas cumplan las dos primeras pueden, incluso,  mejorar la velocidad de procesamiento, y  algunos algoritmos como SVM y Regresión Lineal (con regularización) funcionan mejor cuando las cumplen.   \n",
    "\n",
    "Scikit-Learn nos provee de herramientas para conseguir las tres cosas, pero por ahora veremos las dos primeras:\n",
    "\n",
    "- Que la media sea 0  \n",
    "- Que el  desvío standard sea 1,  $\\sigma = 1$.  \n",
    "\n",
    "Cómo se consigue? Es muy sencillo:  \n",
    "\n",
    "- Si queremos que la media de una variable sea 0, simplemente restamos la media a cada uno de sus valores. De esta manera la \"nueva media\" será 0; se suele decir que ahora la variable \"está centrada en 0\" \n",
    "\n",
    "- Si queremos que el desvío standard sea 1, entonces simplemente dividimos cada uno de sus valores por el valor del desvío standard obtenido para todos sus valores. De esta manera el desvío standard de la variable transformada valdrá 1.  \n",
    "\n",
    "En definitiva el algoritmo para estandarizar será:  \n",
    "\n",
    "\n",
    "$$ x_{iestandarizado}=\\frac{x_i -x_{media}} {\\sigma}$$\n",
    "\n",
    "\n",
    "- Es de notar que la variable así transformada, tendrá media = 0 y $\\sigma = 1$, pero sus valores **no** estarán en el intervalo [0,1].  \n",
    "\n",
    "Afortunadamente Scikit-Learn nos provee de una herramienta para **estandarizar** sin que tengamos que efectuar las cuentas.   \n",
    "\n",
    "\n",
    "Por las dudas aclaramos que si nuestro propósito es pronosticar:  \n",
    "\n",
    "> **Estandarizaremos usando la media y el $\\sigma$ del X_train** y luego aplicaremos la estandarización al X_train y al X_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización con Scikit - Learn con StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La documentación de StandardScaler se encuentra en: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  \n",
    "\n",
    "\n",
    "Su sintaxis es la siguiente:  \n",
    "\n",
    "~~~\n",
    "sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
    "~~~\n",
    "\n",
    "Las opciones principales son las siguientes:  \n",
    "\n",
    "- with_mean=True, la opción por defecto, significa que resta la media a cada uno de los valores, si se establece en False no lo hace. \n",
    "\n",
    "- with_std=True, la opción por defecto, significa que divide por el desvío standard $\\sigma$, si se establece en False, no lo hace.  \n",
    "\n",
    "En general dejaremos estas opciones en sus valores por defecto.  \n",
    "\n",
    "Veamos su uso básico:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:10:51.842787Z",
     "start_time": "2020-06-22T15:10:51.832819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x1\n",
       "0   10\n",
       "1   90\n",
       "2   80\n",
       "3   70\n",
       "4  100\n",
       "5   10"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:13:51.205401Z",
     "start_time": "2020-06-22T15:13:51.196464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.36930639],\n",
       "       [ 0.82158384],\n",
       "       [ 0.54772256],\n",
       "       [ 0.27386128],\n",
       "       [ 1.09544512],\n",
       "       [-1.36930639]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creamos el \"estandarizador\"\n",
    "estandard = StandardScaler()\n",
    "\n",
    "# lo entrenamos con fit. Simplmente calcula la media y el desvio de los datos que le pasamos\n",
    "estandard.fit(df)\n",
    "\n",
    "# lo aplicamos\n",
    "\n",
    "estandard.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusión de la \"estandarización\"en el flujo de Machine Learning\n",
    "\n",
    "Se utiliza de la misma manera que el normalizador visto anteriormente.   \n",
    "Repitamos el mismo ejemplo anterior, pero ahora estandaricemos la variable numérica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:27:07.744499Z",
     "start_time": "2020-06-22T15:27:07.740510Z"
    }
   },
   "outputs": [],
   "source": [
    "t_estandard=(\"estandarizador\",StandardScaler(),['peso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo demás es igual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:27:09.703403Z",
     "start_time": "2020-06-22T15:27:09.698417Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dummy para otras_enfermedades:\n",
    "t_nominal=(\"onehot\",OneHotEncoder(sparse=False),['otras_enfermedades'])\n",
    "# Ordinal con asignación de orden para envergadura,altura y globulos_blancos\n",
    "t_ordinal_enc=(\"ordinal_enc\", OrdinalEncoder(categories=[['pequeño','medio','grande'],\n",
    "                                                         ['bajo','medio','alto'],['bajo','normal','alto']]), \n",
    "               ['envergadura','altura','globulos_blancos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:07.840899Z",
     "start_time": "2020-06-22T15:29:07.836890Z"
    }
   },
   "outputs": [],
   "source": [
    "transformador_columnas = ColumnTransformer(transformers=[t_estandard,t_nominal,t_ordinal_enc],\n",
    "                                                       remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:09.340148Z",
     "start_time": "2020-06-22T15:29:09.328174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entrenamos en el X_train\n",
    "transformador_columnas.fit(X_train)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:17.700880Z",
     "start_time": "2020-06-22T15:29:17.690907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.55614273,  0.        ,  1.        ,  0.        ,  2.        ,\n",
       "         1.        ],\n",
       "       [ 0.70096519,  1.        ,  0.        ,  1.        ,  1.        ,\n",
       "         2.        ],\n",
       "       [ 0.21028956,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-0.08411582,  0.        ,  1.        ,  0.        ,  2.        ,\n",
       "         2.        ],\n",
       "       [ 0.99537057,  0.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ],\n",
       "       [ 1.0935057 ,  1.        ,  0.        ,  1.        ,  1.        ,\n",
       "         1.        ],\n",
       "       [-1.35987247,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         2.        ]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicamos sobre el X_train\n",
    "X_train_transformado=transformador_columnas.transform(X_train)\n",
    "X_train_transformado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:29:23.034663Z",
     "start_time": "2020-06-22T15:29:23.022654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.66366773,  1.        ,  0.        ,  2.        ,  1.        ,\n",
       "         2.        ],\n",
       "       [-0.08411582,  1.        ,  0.        ,  1.        ,  0.        ,\n",
       "         1.        ],\n",
       "       [-2.24308862,  0.        ,  1.        ,  0.        ,  2.        ,\n",
       "         2.        ],\n",
       "       [ 1.97672184,  0.        ,  1.        ,  2.        ,  1.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicamos sobre el X_test\n",
    "X_test_transformado=transformador_columnas.transform(X_test)\n",
    "X_test_transformado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:30:28.942595Z",
     "start_time": "2020-06-22T15:30:28.929632Z"
    }
   },
   "source": [
    "Ahora tenemos listos nuestros datos para pasar a cualquier modelo de Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La pregunta del millón: normalizar o estandarizar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qué conviene más normalizar o estandarizar para pronosticar?   \n",
    "\n",
    "Lamentablemente no existe una respuestas única, en algunos casos será mejor una técnica y en otros casos, la otra. Depende tanto del modelo que pensemos utilizar como de los mismos datos.    \n",
    "\n",
    "Algunos tips:  \n",
    "\n",
    "- Si vamos a usar algún modelo que se base en la medida de distancias, como kNN y sus derivados, **normalizar** puede funcionar mejor.  \n",
    "\n",
    "  \n",
    "- Si el proceso de minimización de la función de costo J se hará con el método de **Gradient Descent**, **normalizar** puede funcionar mejor. El problema es que hay muchos métodos que pueden usar tanto Gradient Descent como otros métodos para minimizar, así que habrá que leer la documentación de la implementación del algoritmo, pero:  \n",
    "\n",
    "- las **Redes Neuronales** generalmente se implementan con Gradient Descent, así que puuede ser que funcionen mejor **normalizando**.   \n",
    "- Lo mismo ocurre con SVM.   \n",
    "- En cambio Regresión Lineal generalmente se minimiza con un método matricial, asi que posiblemente convenga **estandarizar** ( por ejemplo Scikit-Learn por defecto minimiza de esta manera, pero se puede configurar para que use Gradient Descent).  \n",
    "\n",
    "- Los árboles y sus derivados son inmunes a los problemas de escala, así que quizá convenga **estandarizar** o no transformar.\n",
    "\n",
    "\n",
    "En la práctica y según la disponibilidad de tiempo, es bueno probar las 3 posibilidades: sin modificar, normalizando y estandarizando, por ejemplo eligiendo un modelo cualquiera y corriéndolo en un Validation Set para ver si hay diferencias a favor de algunos de los procedimientos.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Tabla de Contenidos",
   "title_sidebar": "Contenidos",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
